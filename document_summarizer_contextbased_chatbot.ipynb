{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from langchain.chains import load_summarize_chain, LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatOllama\n",
    "ollama_api_key = \"http://localhost:11434/v1/chat/completions\"\n",
    "llama_model = OllamaLLM(model=\"llama3.2:latest\", api_key=ollama_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1: Load document using unstructuredFileLoader\n",
    "def load_document(file_path):\n",
    "    loader = UnstructuredFileLoader(file_path)\n",
    "    document = loader.load()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 2: Split documents into chunks using RecursiveCharacterTextSplitter\n",
    "def split_document(document):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(document)\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 3: Summarize document using load_summarize_chain\n",
    "def summarize_document(chunks):\n",
    "    summarize_chain = load_summarize_chain(OllamaLLM(model=\"llama3.2:latest\", api_key=ollama_api_key))\n",
    "    summary = summarize_chain.run(chunks)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 4: Create QA chain\n",
    "def create_qa_chain():\n",
    "    qa_chain = load_qa_chain(OllamaLLM(model=\"llama3.2:latest\", api_key=ollama_api_key), chain_type=\"map_reduce\")\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Based only on the following document summary, answer questions strictly within its context. \"\n",
    "    \"If a question is outside the scope, respond with: \"\n",
    "    \"‘This information is not in the document context, but I can provide general knowledge if you like.’\\n\\n\"\n",
    "    \"Document Summary: {document_summary}\\n\\nUser Question: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 5: Ask question from user and get an answer in the context of the document\n",
    "def ask_question(qa_chain, question, document_summary):\n",
    "\n",
    "    formatted_prompt = prompt_template.format(document_summary=document_summary, question=question)\n",
    "\n",
    "    answer = qa_chain.run(input_documents=document_summary, question=question)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the interactive chatbot\n",
    "def interactive_chatbot(qa_chain, document_summary):\n",
    "    print(\"Welcome to the BZINT document Chatbot! Type 'exit' to stop.\")\n",
    "\n",
    "    conversation_history = []\n",
    "\n",
    "    while True:\n",
    "        # Get user input from the prompt\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        # Exit condition\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add user input to conversation history\n",
    "        conversation_history.append(f\"You: {user_input}\")\n",
    "\n",
    "        # Create a prompt string from the conversation history and document summary\n",
    "        prompt = f\"Document Summary: {document_summary}\\n\\n\"  # Include the document summary at the start\n",
    "        prompt += \"\\n\".join(conversation_history)  # Append the conversation history\n",
    "\n",
    "        # Send the prompt to the Ollama model for a response\n",
    "        response = llama_model.invoke(prompt)\n",
    "\n",
    "        # Add AI response to conversation history\n",
    "        conversation_history.append(f\"AI: {response}\")\n",
    "\n",
    "        print(f\"You: {user_input}\\nAI: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Document: \n",
      "Here is a concise summary:\n",
      "\n",
      "The SeniorSight project developed an interactive dashboard to visualize trends in Malaysia's ageing population and support systems. The dashboard was successfully completed, delivering a comprehensive tool for stakeholders to make informed decisions and improve planning for the elderly. Key strengths include user-friendly design, robust data integration, and accessibility enhancements. Future recommendations include integrating real-time data feeds, enhancing predictive analytics, and collaborative partnerships with healthcare providers, NGOs, and academic institutions.\n",
      "Welcome to the BZINT document Chatbot! Type 'exit' to stop.\n",
      "You: who is this dashboard for?\n",
      "AI: The SeniorSight project's interactive dashboard appears to be designed for stakeholders related to the elderly population in Malaysia, including:\n",
      "\n",
      "1. Government officials involved in planning and policy-making.\n",
      "2. Healthcare professionals working with older adults.\n",
      "3. Non-governmental organization (NGO) representatives focused on senior welfare and support services.\n",
      "4. Academic researchers studying demographics and aging trends.\n",
      "\n",
      "These groups can benefit from having access to accurate and up-to-date information about the aging population, which can inform their decision-making and planning efforts.\n",
      "You: what is RAG?\n",
      "AI: There is no mention of \"RAG\" in the provided summary. It's possible that you might be referring to a specific concept or acronym related to the SeniorSight project, but without further context, I couldn't determine what RAG stands for. If you could provide more information or clarify what RAG refers to, I'd be happy to try and help you further.\n",
      "You: the dashboard is developed using what software?\n",
      "AI: Unfortunately, the provided summary does not mention the specific software used to develop the SeniorSight project's interactive dashboard. However, based on common practices in data visualization and analytics, it is likely that the dashboard was built using a combination of tools and technologies such as:\n",
      "\n",
      "1. Data visualization libraries (e.g., D3.js, Plotly)\n",
      "2. Web development frameworks (e.g., React, Angular)\n",
      "3. Database management systems (e.g., MySQL, MongoDB)\n",
      "4. Programming languages (e.g., Python, R)\n",
      "\n",
      "If you need more specific information about the software used to develop the dashboard, you may want to try searching for additional resources or contacting the project team directly.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Path to the document to be uploaded\n",
    "    file_path = \"C:\\\\Users\\\\nanadhirah\\\\Downloads\\\\SENIORSIGHT_THESIS_NUR AIN NADHIRAH ZAWAYI_2021829416_organized.pdf\" \n",
    "    \n",
    "    # Load the document\n",
    "    document = load_document(file_path)\n",
    "\n",
    "    # Split document into chunks\n",
    "    chunks = split_document(document)\n",
    "\n",
    "    # Summarize document\n",
    "    summary = summarize_document(chunks)\n",
    "    print(\"Summary of Document: \")\n",
    "    print(summary)\n",
    "\n",
    "    # Create QA chain\n",
    "    qa_chain = create_qa_chain()\n",
    "\n",
    "    # Ask a question (optional, just to check QA)\n",
    "    #question = \"Summarize this document\"\n",
    "    #answer = summarize_document(chunks)\n",
    "    #print(f\"Answer to the Question: {answer}\")\n",
    "\n",
    "    # Start the interactive chatbot session with document context\n",
    "    interactive_chatbot(qa_chain, summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
